{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Raven Jiang MSIS549_HW1_newswires.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0lDkFZq3ohI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        },
        "outputId": "7058ce98-27ee-4bea-c02b-81f006a2f055"
      },
      "source": [
        "%tensorflow_version 1.14\n",
        "%matplotlib inline\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "\n",
        "import keras\n",
        "keras.__version__\n",
        "\n",
        "!pip install numpy==1.16.1\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `1.14`. This will be interpreted as: `1.x`.\n",
            "\n",
            "\n",
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting numpy==1.16.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/bf/4981bcbee43934f0adb8f764a1e70ab0ee5a448f6505bd04a87a2fda2a8b/numpy-1.16.1-cp36-cp36m-manylinux1_x86_64.whl (17.3MB)\n",
            "\u001b[K     |████████████████████████████████| 17.3MB 236kB/s \n",
            "\u001b[31mERROR: tensorflow 1.15.2 has requirement gast==0.2.2, but you'll have gast 0.3.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Found existing installation: numpy 1.18.2\n",
            "    Uninstalling numpy-1.18.2:\n",
            "      Successfully uninstalled numpy-1.18.2\n",
            "Successfully installed numpy-1.16.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bu5jf9gW3ohR",
        "colab_type": "text"
      },
      "source": [
        "# Classifying newswires: a multi-class classification example\n",
        "\n",
        "In this homework, we will build a network to classify Reuters newswires into 46 different mutually-exclusive topics. Since we have many \n",
        "classes, this problem is an instance of \"multi-class classification\", and since each data point should be classified into only one \n",
        "category, the problem is more specifically an instance of \"single-label, multi-class classification\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-s49gXW3ohT",
        "colab_type": "text"
      },
      "source": [
        "## The Reuters dataset\n",
        "\n",
        "\n",
        "We will be working with the _Reuters dataset_, a set of short newswires and their topics, published by Reuters in 1986. It's a very simple, \n",
        "widely used toy dataset for text classification. There are 46 different topics; some topics are more represented than others, but each \n",
        "topic has at least 10 examples in the training set.\n",
        "\n",
        "Like IMDB and MNIST, the Reuters dataset comes packaged as part of Keras. Let's take a look right away:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjSiBNZS3ohU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.datasets import reuters\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VRrIjNI3ohX",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Like with the IMDB dataset, the argument `num_words=10000` restricts the data to the 10,000 most frequently occurring words found in the \n",
        "data.\n",
        "\n",
        "We have 8,982 training examples and 2,246 test examples:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMdiLzvF3ohf",
        "colab_type": "text"
      },
      "source": [
        "As with the IMDB reviews, each example is a list of integers (word indices):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QriVgMSe3ohg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bc5b4b46-21a6-4137-f7ca-5bdefde841aa"
      },
      "source": [
        "train_data[0]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1,\n",
              " 2,\n",
              " 2,\n",
              " 8,\n",
              " 43,\n",
              " 10,\n",
              " 447,\n",
              " 5,\n",
              " 25,\n",
              " 207,\n",
              " 270,\n",
              " 5,\n",
              " 3095,\n",
              " 111,\n",
              " 16,\n",
              " 369,\n",
              " 186,\n",
              " 90,\n",
              " 67,\n",
              " 7,\n",
              " 89,\n",
              " 5,\n",
              " 19,\n",
              " 102,\n",
              " 6,\n",
              " 19,\n",
              " 124,\n",
              " 15,\n",
              " 90,\n",
              " 67,\n",
              " 84,\n",
              " 22,\n",
              " 482,\n",
              " 26,\n",
              " 7,\n",
              " 48,\n",
              " 4,\n",
              " 49,\n",
              " 8,\n",
              " 864,\n",
              " 39,\n",
              " 209,\n",
              " 154,\n",
              " 6,\n",
              " 151,\n",
              " 6,\n",
              " 83,\n",
              " 11,\n",
              " 15,\n",
              " 22,\n",
              " 155,\n",
              " 11,\n",
              " 15,\n",
              " 7,\n",
              " 48,\n",
              " 9,\n",
              " 4579,\n",
              " 1005,\n",
              " 504,\n",
              " 6,\n",
              " 258,\n",
              " 6,\n",
              " 272,\n",
              " 11,\n",
              " 15,\n",
              " 22,\n",
              " 134,\n",
              " 44,\n",
              " 11,\n",
              " 15,\n",
              " 16,\n",
              " 8,\n",
              " 197,\n",
              " 1245,\n",
              " 90,\n",
              " 67,\n",
              " 52,\n",
              " 29,\n",
              " 209,\n",
              " 30,\n",
              " 32,\n",
              " 132,\n",
              " 6,\n",
              " 109,\n",
              " 15,\n",
              " 17,\n",
              " 12]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIvaOPin3ohj",
        "colab_type": "text"
      },
      "source": [
        "Here's how you can decode it back to words, in case you are curious:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZg6ME3Y3ohk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "54433985-3803-4c3f-e4b3-aa4c89e90ae4"
      },
      "source": [
        "word_index = reuters.get_word_index()\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "# Note that our indices were offset by 3\n",
        "# because 0, 1 and 2 are reserved indices for \"padding\", \"start of sequence\", and \"unknown\".\n",
        "decoded_newswire = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])\n",
        "\n",
        "print(\"The decoded text:\" + decoded_newswire)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/reuters_word_index.json\n",
            "557056/550378 [==============================] - 1s 2us/step\n",
            "The decoded text:? ? ? said as a result of its december acquisition of space co it expects earnings per share in 1987 of 1 15 to 1 30 dlrs per share up from 70 cts in 1986 the company said pretax net should rise to nine to 10 mln dlrs from six mln dlrs in 1986 and rental operation revenues to 19 to 22 mln dlrs from 12 5 mln dlrs it said cash flow per share this year should be 2 50 to three dlrs reuter 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yntI1JS33oho",
        "colab_type": "text"
      },
      "source": [
        "The label associated with an example is an integer between 0 and 45: a topic index."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swMqUzqz3ohq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4dbe824d-21e8-476a-c127-d3e3600f9815"
      },
      "source": [
        "train_labels[0]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsLn40NK3oht",
        "colab_type": "text"
      },
      "source": [
        "## Preparing the data\n",
        "\n",
        "We can vectorize the data with the exact same code as in our previous example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDol7z_43ohu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def vectorize_sequences(sequences, dimension=10000):\n",
        "    results = np.zeros((len(sequences), dimension))\n",
        "    for i, sequence in enumerate(sequences):\n",
        "        results[i, sequence] = 1.\n",
        "    return results\n",
        "\n",
        "# Our vectorized training data\n",
        "x_train = vectorize_sequences(train_data)\n",
        "# Our vectorized test data\n",
        "x_test = vectorize_sequences(test_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pte7Qr63ohv",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Use \"one-hot\" encoding to vectorize the labels. One-hot encoding is a widely used format for categorical data, also called \"categorical encoding\". In our case, one-hot encoding of our labels consists in embedding each label as an all-zero vector with a 1 in the place of the label index. Note that there is a built-in way to do this in Keras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XO_SmjOD3ohy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "one_hot_train_labels = to_categorical(train_labels)\n",
        "one_hot_test_labels = to_categorical(test_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMvTFcc03ohz",
        "colab_type": "text"
      },
      "source": [
        "## Building our network\n",
        "\n",
        "\n",
        "This topic classification problem looks very similar to our previous movie review classification problem: in both cases, we are trying to \n",
        "classify short snippets of text. There is however a new constraint here: the number of output classes has gone from 2 to 46, i.e. the \n",
        "dimensionality of the output space is much larger. \n",
        "\n",
        "In a stack of `Dense` layers like what we were using, each layer can only access information present in the output of the previous layer. \n",
        "If one layer drops some information relevant to the classification problem, this information can never be recovered by later layers: each \n",
        "layer can potentially become an \"information bottleneck\". In our previous example, we were using 16-dimensional intermediate layers, but a \n",
        "16-dimensional space may be too limited to learn to separate 46 different classes: such small layers may act as information bottlenecks, \n",
        "permanently dropping relevant information.\n",
        "\n",
        "For this reason we will use larger layers. Let's go with 64 units:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdhwRTjx3ohz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "f5facad3-7111-4385-f26f-8df01b07a646"
      },
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "# TODO: specify the architecture of the model.\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(46, activation='softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_4 (Dense)              (None, 64)                640064    \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 46)                2990      \n",
            "=================================================================\n",
            "Total params: 647,214\n",
            "Trainable params: 647,214\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-Aj666N3oh1",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "There are two other things you should note about this architecture:\n",
        "\n",
        "* We are ending the network with a `Dense` layer of size 46. This means that for each input sample, our network will output a \n",
        "46-dimensional vector. Each entry in this vector (each dimension) will encode a different output class.\n",
        "* The last layer uses a `softmax` activation. You have already seen this pattern in the MNIST example. It means that the network will \n",
        "output a _probability distribution_ over the 46 different output classes, i.e. for every input sample, the network will produce a \n",
        "46-dimensional output vector where `output[i]` is the probability that the sample belongs to class `i`. The 46 scores will sum to 1.\n",
        "\n",
        "The best loss function to use in this case is `categorical_crossentropy`. It measures the distance between two probability distributions: \n",
        "in our case, between the probability distribution output by our network, and the true distribution of the labels. By minimizing the \n",
        "distance between these two distributions, we train our network to output something as close as possible to the true labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhmA6zNZ3oh1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: compile the model you just built\n",
        "\n",
        "from keras import optimizers\n",
        "from keras import losses\n",
        "from keras import metrics\n",
        "\n",
        "model.compile(optimizer=optimizers.RMSprop(lr=0.001),\n",
        "              loss=losses.categorical_crossentropy,\n",
        "              metrics=[metrics.categorical_accuracy])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGB_cp8B3oh2",
        "colab_type": "text"
      },
      "source": [
        "## Validating our approach\n",
        "\n",
        "Let's set apart 1,000 samples in our training data to use as a validation set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3eyzgOA3oh2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_val = x_train[:1000]\n",
        "partial_x_train = x_train[1000:]\n",
        "\n",
        "y_val = one_hot_train_labels[:1000]\n",
        "partial_y_train = one_hot_train_labels[1000:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIFUZkt43oh3",
        "colab_type": "text"
      },
      "source": [
        "Now let's train our network for 20 epochs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MM57jbod3oh3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734
        },
        "outputId": "ee6598f0-2258-430e-d42b-b29f9e10ea54"
      },
      "source": [
        "# TODO: fit the model with the training dataset and provide validation data \n",
        "# to help check overfitting\n",
        "history = history = model.fit(partial_x_train, partial_y_train, epochs=20, \n",
        "                              batch_size=512, validation_data=(x_val, y_val))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 7982 samples, validate on 1000 samples\n",
            "Epoch 1/20\n",
            "7982/7982 [==============================] - 2s 197us/step - loss: 2.6075 - categorical_accuracy: 0.5159 - val_loss: 1.7086 - val_categorical_accuracy: 0.6380\n",
            "Epoch 2/20\n",
            "7982/7982 [==============================] - 1s 127us/step - loss: 1.4010 - categorical_accuracy: 0.7136 - val_loss: 1.2942 - val_categorical_accuracy: 0.7230\n",
            "Epoch 3/20\n",
            "7982/7982 [==============================] - 1s 129us/step - loss: 1.0285 - categorical_accuracy: 0.7783 - val_loss: 1.1139 - val_categorical_accuracy: 0.7740\n",
            "Epoch 4/20\n",
            "7982/7982 [==============================] - 1s 127us/step - loss: 0.7992 - categorical_accuracy: 0.8305 - val_loss: 1.0352 - val_categorical_accuracy: 0.7810\n",
            "Epoch 5/20\n",
            "7982/7982 [==============================] - 1s 126us/step - loss: 0.6334 - categorical_accuracy: 0.8656 - val_loss: 0.9732 - val_categorical_accuracy: 0.8130\n",
            "Epoch 6/20\n",
            "7982/7982 [==============================] - 1s 127us/step - loss: 0.5047 - categorical_accuracy: 0.8969 - val_loss: 0.9167 - val_categorical_accuracy: 0.8150\n",
            "Epoch 7/20\n",
            "7982/7982 [==============================] - 1s 127us/step - loss: 0.4072 - categorical_accuracy: 0.9157 - val_loss: 0.9046 - val_categorical_accuracy: 0.8190\n",
            "Epoch 8/20\n",
            "7982/7982 [==============================] - 1s 128us/step - loss: 0.3281 - categorical_accuracy: 0.9323 - val_loss: 0.9310 - val_categorical_accuracy: 0.8140\n",
            "Epoch 9/20\n",
            "7982/7982 [==============================] - 1s 128us/step - loss: 0.2751 - categorical_accuracy: 0.9382 - val_loss: 0.8978 - val_categorical_accuracy: 0.8180\n",
            "Epoch 10/20\n",
            "7982/7982 [==============================] - 1s 128us/step - loss: 0.2307 - categorical_accuracy: 0.9459 - val_loss: 0.9181 - val_categorical_accuracy: 0.8190\n",
            "Epoch 11/20\n",
            "7982/7982 [==============================] - 1s 128us/step - loss: 0.1999 - categorical_accuracy: 0.9511 - val_loss: 0.9525 - val_categorical_accuracy: 0.8080\n",
            "Epoch 12/20\n",
            "7982/7982 [==============================] - 1s 127us/step - loss: 0.1793 - categorical_accuracy: 0.9518 - val_loss: 1.0069 - val_categorical_accuracy: 0.8140\n",
            "Epoch 13/20\n",
            "7982/7982 [==============================] - 1s 128us/step - loss: 0.1626 - categorical_accuracy: 0.9541 - val_loss: 1.0047 - val_categorical_accuracy: 0.8100\n",
            "Epoch 14/20\n",
            "7982/7982 [==============================] - 1s 128us/step - loss: 0.1476 - categorical_accuracy: 0.9560 - val_loss: 0.9867 - val_categorical_accuracy: 0.8060\n",
            "Epoch 15/20\n",
            "7982/7982 [==============================] - 1s 125us/step - loss: 0.1422 - categorical_accuracy: 0.9562 - val_loss: 1.0032 - val_categorical_accuracy: 0.8040\n",
            "Epoch 16/20\n",
            "7982/7982 [==============================] - 1s 126us/step - loss: 0.1307 - categorical_accuracy: 0.9567 - val_loss: 1.0273 - val_categorical_accuracy: 0.8120\n",
            "Epoch 17/20\n",
            "7982/7982 [==============================] - 1s 126us/step - loss: 0.1219 - categorical_accuracy: 0.9580 - val_loss: 1.0478 - val_categorical_accuracy: 0.8140\n",
            "Epoch 18/20\n",
            "7982/7982 [==============================] - 1s 125us/step - loss: 0.1178 - categorical_accuracy: 0.9575 - val_loss: 1.0473 - val_categorical_accuracy: 0.8130\n",
            "Epoch 19/20\n",
            "7982/7982 [==============================] - 1s 126us/step - loss: 0.1154 - categorical_accuracy: 0.9568 - val_loss: 1.0938 - val_categorical_accuracy: 0.8140\n",
            "Epoch 20/20\n",
            "7982/7982 [==============================] - 1s 126us/step - loss: 0.1152 - categorical_accuracy: 0.9563 - val_loss: 1.1545 - val_categorical_accuracy: 0.7940\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rq5P0gWQ3oh5",
        "colab_type": "text"
      },
      "source": [
        "Let's display its loss and accuracy curves to help identify when it starts to overfit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIRFHE0U3oh5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "a40004a8-b552-4ee0-8c3b-6f1a014147f8"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# TODO: plot the model loss on both training and validation data. \n",
        "\n",
        "acc = history.history['categorical_accuracy']\n",
        "val_acc = history.history['val_categorical_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f04f0cbc198>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M69rtt143oh6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "6e44682e-b888-47a7-cb16-8fd9f7edf444"
      },
      "source": [
        "# TODO: plot prediction accuracy on both training and validation data. \n",
        "plt.clf()\n",
        "acc = history.history['categorical_accuracy']\n",
        "val_acc = history.history['val_categorical_accuracy']\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUarkgZd3oh7",
        "colab_type": "text"
      },
      "source": [
        "It seems that the network starts overfitting after certain epochs. Let's train a new network from scratch for fewer epochs before it starts overfitting, then let's evaluate it on the test set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IciZ2dMP3oh8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "7ac266a0-2856-4991-da2b-69326bf2b8db"
      },
      "source": [
        "# TODO: retrain the model with the fewer epoches to avoid overfitting\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(46, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer = 'rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.fit( partial_x_train, partial_y_train, epochs=9, batch_size=512, validation_data=(x_val, y_val))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 7982 samples, validate on 1000 samples\n",
            "Epoch 1/9\n",
            "7982/7982 [==============================] - 1s 147us/step - loss: 2.6893 - accuracy: 0.5563 - val_loss: 1.7410 - val_accuracy: 0.6570\n",
            "Epoch 2/9\n",
            "7982/7982 [==============================] - 1s 129us/step - loss: 1.3999 - accuracy: 0.7100 - val_loss: 1.3073 - val_accuracy: 0.7200\n",
            "Epoch 3/9\n",
            "7982/7982 [==============================] - 1s 131us/step - loss: 1.0305 - accuracy: 0.7798 - val_loss: 1.1398 - val_accuracy: 0.7510\n",
            "Epoch 4/9\n",
            "7982/7982 [==============================] - 1s 128us/step - loss: 0.8135 - accuracy: 0.8312 - val_loss: 1.0338 - val_accuracy: 0.7950\n",
            "Epoch 5/9\n",
            "7982/7982 [==============================] - 1s 130us/step - loss: 0.6431 - accuracy: 0.8690 - val_loss: 0.9793 - val_accuracy: 0.7930\n",
            "Epoch 6/9\n",
            "7982/7982 [==============================] - 1s 129us/step - loss: 0.5151 - accuracy: 0.8974 - val_loss: 0.9344 - val_accuracy: 0.8040\n",
            "Epoch 7/9\n",
            "7982/7982 [==============================] - 1s 130us/step - loss: 0.4142 - accuracy: 0.9161 - val_loss: 0.9216 - val_accuracy: 0.8130\n",
            "Epoch 8/9\n",
            "7982/7982 [==============================] - 1s 132us/step - loss: 0.3362 - accuracy: 0.9305 - val_loss: 0.9093 - val_accuracy: 0.8140\n",
            "Epoch 9/9\n",
            "7982/7982 [==============================] - 1s 130us/step - loss: 0.2744 - accuracy: 0.9406 - val_loss: 0.9321 - val_accuracy: 0.8080\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f04f2b266a0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQ3VcXyx3oh_",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Your model should reach an accuracy of ~78%. With a balanced binary classification problem, the accuracy reached by a purely random classifier \n",
        "would be 50%, but in our case it is closer to 19%, so our results seem pretty good, at least when compared to a random baseline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEzjb1m33oiA",
        "colab_type": "text"
      },
      "source": [
        "## Generating predictions on new data\n",
        "\n",
        "We can verify that the `predict` method of our model instance returns a probability distribution over all 46 topics. Let's generate topic \n",
        "predictions for all of the test data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIuTWvUq3oiB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "947bafa3-c5d0-44ee-a766-6db6beef9cd7"
      },
      "source": [
        "# TODO: use the learnt neural network to make a prediction on \n",
        "# the test data \n",
        "predict = model.predict(x_test)\n",
        "predict"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[4.31233002e-06, 6.90595925e-06, 1.36663548e-05, ...,\n",
              "        1.44081550e-05, 5.63186120e-07, 1.78323376e-06],\n",
              "       [1.86676520e-03, 7.83132166e-02, 2.02660868e-03, ...,\n",
              "        8.35797051e-04, 1.42953177e-05, 3.60932492e-04],\n",
              "       [1.64196678e-02, 8.86717081e-01, 3.69732617e-03, ...,\n",
              "        5.98662125e-04, 1.03705686e-04, 3.97050841e-04],\n",
              "       ...,\n",
              "       [7.71724990e-06, 1.89128095e-05, 6.49017456e-05, ...,\n",
              "        2.72528414e-05, 1.85898830e-06, 1.62415915e-06],\n",
              "       [5.80018561e-04, 7.57178990e-03, 3.17204418e-03, ...,\n",
              "        5.20201284e-04, 9.58092278e-05, 5.97224804e-04],\n",
              "       [2.15016829e-04, 2.88763613e-01, 6.07406115e-03, ...,\n",
              "        2.66141212e-03, 4.82790289e-04, 1.38231000e-04]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1LzmGXx3oiB",
        "colab_type": "text"
      },
      "source": [
        "Each entry in `predictions` is a vector of length 46. **The** largest entry is the predicted class, i.e. the class with the highest probability:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3RyKAsw3oiB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "129e6660-8c6a-4f92-981d-e4cc893df102"
      },
      "source": [
        "# TODO: evaluate model performan in terms of accuracy on prediction against the ground truth.\n",
        "test_loss, test_acc=model.evaluate(x_test,predict)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2246/2246 [==============================] - 0s 111us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHHat87YBoKC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3398fb32-1104-4628-a758-abfe129f79d8"
      },
      "source": [
        "predict.shape"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2246, 46)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRn5VgMFBuD-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f45707b2-c4f7-45a9-df3a-63a760f6cfa2"
      },
      "source": [
        "print('test_acc:', test_acc)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test_acc: 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pw3GlSYG3oiH",
        "colab_type": "text"
      },
      "source": [
        "## On the importance of having sufficiently large intermediate layers\n",
        "\n",
        "\n",
        "We mentioned earlier that since our final outputs were 46-dimensional, we should avoid intermediate layers with much less than 46 hidden \n",
        "units. Now let's try to see what happens when we introduce an information bottleneck by having intermediate layers significantly less than \n",
        "46-dimensional, e.g. 4-dimensional."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emfKokRe3oiH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 751
        },
        "outputId": "90f50f56-90dc-4d42-816c-7d3d5fdef670"
      },
      "source": [
        "# TODO: build a neural network with 4 neuron units in the hidden layer with the validation data \n",
        "# and evaluate its performance on the test data.\n",
        "model_2 = models.Sequential()\n",
        "model_2.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
        "model_2.add(layers.Dense(4, activation='relu'))\n",
        "model_2.add(layers.Dense(46, activation='softmax')) \n",
        "\n",
        "model_2.compile( optimizer = 'rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model_2.fit(partial_x_train, partial_y_train, epochs=20, batch_size=512, validation_data=(x_val, y_val))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 7982 samples, validate on 1000 samples\n",
            "Epoch 1/20\n",
            "7982/7982 [==============================] - 1s 174us/step - loss: 3.5881 - accuracy: 0.1131 - val_loss: 3.3488 - val_accuracy: 0.2700\n",
            "Epoch 2/20\n",
            "7982/7982 [==============================] - 1s 128us/step - loss: 3.1625 - accuracy: 0.2717 - val_loss: 3.0549 - val_accuracy: 0.2680\n",
            "Epoch 3/20\n",
            "7982/7982 [==============================] - 1s 128us/step - loss: 2.8639 - accuracy: 0.2767 - val_loss: 2.8414 - val_accuracy: 0.2680\n",
            "Epoch 4/20\n",
            "7982/7982 [==============================] - 1s 126us/step - loss: 2.6470 - accuracy: 0.2864 - val_loss: 2.6895 - val_accuracy: 0.2770\n",
            "Epoch 5/20\n",
            "7982/7982 [==============================] - 1s 128us/step - loss: 2.4723 - accuracy: 0.2998 - val_loss: 2.5592 - val_accuracy: 0.2820\n",
            "Epoch 6/20\n",
            "7982/7982 [==============================] - 1s 126us/step - loss: 2.3057 - accuracy: 0.3106 - val_loss: 2.4233 - val_accuracy: 0.2920\n",
            "Epoch 7/20\n",
            "7982/7982 [==============================] - 1s 127us/step - loss: 2.1364 - accuracy: 0.3210 - val_loss: 2.2914 - val_accuracy: 0.2950\n",
            "Epoch 8/20\n",
            "7982/7982 [==============================] - 1s 127us/step - loss: 1.9485 - accuracy: 0.3311 - val_loss: 2.1062 - val_accuracy: 0.3060\n",
            "Epoch 9/20\n",
            "7982/7982 [==============================] - 1s 127us/step - loss: 1.7304 - accuracy: 0.3865 - val_loss: 1.8967 - val_accuracy: 0.5170\n",
            "Epoch 10/20\n",
            "7982/7982 [==============================] - 1s 125us/step - loss: 1.4819 - accuracy: 0.6330 - val_loss: 1.6909 - val_accuracy: 0.6100\n",
            "Epoch 11/20\n",
            "7982/7982 [==============================] - 1s 125us/step - loss: 1.2580 - accuracy: 0.6869 - val_loss: 1.5418 - val_accuracy: 0.6490\n",
            "Epoch 12/20\n",
            "7982/7982 [==============================] - 1s 126us/step - loss: 1.1068 - accuracy: 0.7154 - val_loss: 1.4665 - val_accuracy: 0.6600\n",
            "Epoch 13/20\n",
            "7982/7982 [==============================] - 1s 130us/step - loss: 1.0116 - accuracy: 0.7314 - val_loss: 1.4178 - val_accuracy: 0.6680\n",
            "Epoch 14/20\n",
            "7982/7982 [==============================] - 1s 125us/step - loss: 0.9468 - accuracy: 0.7385 - val_loss: 1.4071 - val_accuracy: 0.6730\n",
            "Epoch 15/20\n",
            "7982/7982 [==============================] - 1s 125us/step - loss: 0.8944 - accuracy: 0.7595 - val_loss: 1.4260 - val_accuracy: 0.6850\n",
            "Epoch 16/20\n",
            "7982/7982 [==============================] - 1s 126us/step - loss: 0.8487 - accuracy: 0.7839 - val_loss: 1.4204 - val_accuracy: 0.6950\n",
            "Epoch 17/20\n",
            "7982/7982 [==============================] - 1s 125us/step - loss: 0.8111 - accuracy: 0.7944 - val_loss: 1.4145 - val_accuracy: 0.6970\n",
            "Epoch 18/20\n",
            "7982/7982 [==============================] - 1s 126us/step - loss: 0.7736 - accuracy: 0.8026 - val_loss: 1.4228 - val_accuracy: 0.7010\n",
            "Epoch 19/20\n",
            "7982/7982 [==============================] - 1s 127us/step - loss: 0.7418 - accuracy: 0.8093 - val_loss: 1.4348 - val_accuracy: 0.7070\n",
            "Epoch 20/20\n",
            "7982/7982 [==============================] - 1s 127us/step - loss: 0.7127 - accuracy: 0.8143 - val_loss: 1.4456 - val_accuracy: 0.7120\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f04f2cc73c8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6wV_AR23oiI",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "You should see the model performance drop. This drop is mostly due to the fact that we are now trying to compress a lot of information (enough information to recover the separation hyperplanes of 46 classes) into an intermediate space that is too low-dimensional. The network is able to cram _most_ of the necessary information into these 8-dimensional representations, but not all of it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bFLuR8b3oiI",
        "colab_type": "text"
      },
      "source": [
        "## Try using larger or smaller hidden layers: 32 units, and 128 units, and see if you will be able to improve the model performance on the test data.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1v7pFMGrEik",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 751
        },
        "outputId": "4b58cad4-3af0-45c7-fe38-c049d23f4498"
      },
      "source": [
        "# TODO: Try using larger or smaller hidden layers: 32 units, and 128 units\n",
        "model_3 = models.Sequential()\n",
        "model_3.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
        "model_3.add(layers.Dense(32, activation='relu'))\n",
        "model_3.add(layers.Dense(46, activation='softmax')) \n",
        "\n",
        "model_3.compile( optimizer = 'rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model_3.fit(partial_x_train, partial_y_train, epochs=20, batch_size=512, validation_data=(x_val, y_val))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 7982 samples, validate on 1000 samples\n",
            "Epoch 1/20\n",
            "7982/7982 [==============================] - 1s 145us/step - loss: 2.7263 - accuracy: 0.4703 - val_loss: 1.8805 - val_accuracy: 0.6040\n",
            "Epoch 2/20\n",
            "7982/7982 [==============================] - 1s 125us/step - loss: 1.5398 - accuracy: 0.6770 - val_loss: 1.3931 - val_accuracy: 0.7000\n",
            "Epoch 3/20\n",
            "7982/7982 [==============================] - 1s 127us/step - loss: 1.1390 - accuracy: 0.7605 - val_loss: 1.2267 - val_accuracy: 0.7520\n",
            "Epoch 4/20\n",
            "7982/7982 [==============================] - 1s 127us/step - loss: 0.9095 - accuracy: 0.8103 - val_loss: 1.1004 - val_accuracy: 0.7720\n",
            "Epoch 5/20\n",
            "7982/7982 [==============================] - 1s 127us/step - loss: 0.7364 - accuracy: 0.8522 - val_loss: 1.0143 - val_accuracy: 0.7880\n",
            "Epoch 6/20\n",
            "7982/7982 [==============================] - 1s 126us/step - loss: 0.5982 - accuracy: 0.8815 - val_loss: 0.9815 - val_accuracy: 0.7930\n",
            "Epoch 7/20\n",
            "7982/7982 [==============================] - 1s 127us/step - loss: 0.4921 - accuracy: 0.8980 - val_loss: 0.9429 - val_accuracy: 0.7990\n",
            "Epoch 8/20\n",
            "7982/7982 [==============================] - 1s 127us/step - loss: 0.4040 - accuracy: 0.9173 - val_loss: 0.9242 - val_accuracy: 0.8090\n",
            "Epoch 9/20\n",
            "7982/7982 [==============================] - 1s 129us/step - loss: 0.3346 - accuracy: 0.9286 - val_loss: 0.9173 - val_accuracy: 0.8040\n",
            "Epoch 10/20\n",
            "7982/7982 [==============================] - 1s 128us/step - loss: 0.2840 - accuracy: 0.9352 - val_loss: 0.9357 - val_accuracy: 0.8100\n",
            "Epoch 11/20\n",
            "7982/7982 [==============================] - 1s 130us/step - loss: 0.2428 - accuracy: 0.9435 - val_loss: 0.9359 - val_accuracy: 0.8080\n",
            "Epoch 12/20\n",
            "7982/7982 [==============================] - 1s 125us/step - loss: 0.2073 - accuracy: 0.9501 - val_loss: 0.9709 - val_accuracy: 0.8020\n",
            "Epoch 13/20\n",
            "7982/7982 [==============================] - 1s 128us/step - loss: 0.1877 - accuracy: 0.9508 - val_loss: 0.9596 - val_accuracy: 0.8210\n",
            "Epoch 14/20\n",
            "7982/7982 [==============================] - 1s 128us/step - loss: 0.1668 - accuracy: 0.9524 - val_loss: 1.0096 - val_accuracy: 0.8090\n",
            "Epoch 15/20\n",
            "7982/7982 [==============================] - 1s 127us/step - loss: 0.1533 - accuracy: 0.9543 - val_loss: 0.9569 - val_accuracy: 0.8140\n",
            "Epoch 16/20\n",
            "7982/7982 [==============================] - 1s 129us/step - loss: 0.1411 - accuracy: 0.9565 - val_loss: 0.9720 - val_accuracy: 0.8090\n",
            "Epoch 17/20\n",
            "7982/7982 [==============================] - 1s 128us/step - loss: 0.1356 - accuracy: 0.9553 - val_loss: 1.0084 - val_accuracy: 0.8100\n",
            "Epoch 18/20\n",
            "7982/7982 [==============================] - 1s 125us/step - loss: 0.1257 - accuracy: 0.9560 - val_loss: 1.0628 - val_accuracy: 0.8040\n",
            "Epoch 19/20\n",
            "7982/7982 [==============================] - 1s 128us/step - loss: 0.1172 - accuracy: 0.9583 - val_loss: 1.0724 - val_accuracy: 0.8020\n",
            "Epoch 20/20\n",
            "7982/7982 [==============================] - 1s 126us/step - loss: 0.1148 - accuracy: 0.9539 - val_loss: 1.1020 - val_accuracy: 0.7960\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f04f266ea20>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyr6RyynD4n8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "7d63bba0-7b63-4c12-8edf-10376fec4cb7"
      },
      "source": [
        "result = model_3.evaluate(x_test,one_hot_test_labels)\n",
        "print(result)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2246/2246 [==============================] - 0s 105us/step\n",
            "[1.2403340980821917, 0.7822796106338501]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yEjftXLD-5_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 751
        },
        "outputId": "e4635d6d-c6c8-4c65-8590-82e46b4f37fc"
      },
      "source": [
        "model_4 = models.Sequential()\n",
        "model_4.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
        "model_4.add(layers.Dense(128, activation='relu'))\n",
        "model_4.add(layers.Dense(46, activation='softmax')) \n",
        "\n",
        "model_4.compile( optimizer = 'rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model_4.fit(partial_x_train, partial_y_train, epochs=20, batch_size=512, validation_data=(x_val, y_val))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 7982 samples, validate on 1000 samples\n",
            "Epoch 1/20\n",
            "7982/7982 [==============================] - 1s 155us/step - loss: 2.5410 - accuracy: 0.5366 - val_loss: 1.6175 - val_accuracy: 0.6600\n",
            "Epoch 2/20\n",
            "7982/7982 [==============================] - 1s 130us/step - loss: 1.3504 - accuracy: 0.7096 - val_loss: 1.2560 - val_accuracy: 0.7170\n",
            "Epoch 3/20\n",
            "7982/7982 [==============================] - 1s 132us/step - loss: 1.0183 - accuracy: 0.7772 - val_loss: 1.0997 - val_accuracy: 0.7500\n",
            "Epoch 4/20\n",
            "7982/7982 [==============================] - 1s 134us/step - loss: 0.7909 - accuracy: 0.8289 - val_loss: 1.0096 - val_accuracy: 0.7760\n",
            "Epoch 5/20\n",
            "7982/7982 [==============================] - 1s 132us/step - loss: 0.6127 - accuracy: 0.8711 - val_loss: 0.9237 - val_accuracy: 0.8050\n",
            "Epoch 6/20\n",
            "7982/7982 [==============================] - 1s 131us/step - loss: 0.4797 - accuracy: 0.8980 - val_loss: 0.9015 - val_accuracy: 0.8160\n",
            "Epoch 7/20\n",
            "7982/7982 [==============================] - 1s 133us/step - loss: 0.3793 - accuracy: 0.9217 - val_loss: 0.8780 - val_accuracy: 0.8160\n",
            "Epoch 8/20\n",
            "7982/7982 [==============================] - 1s 132us/step - loss: 0.3090 - accuracy: 0.9325 - val_loss: 0.8791 - val_accuracy: 0.8140\n",
            "Epoch 9/20\n",
            "7982/7982 [==============================] - 1s 133us/step - loss: 0.2537 - accuracy: 0.9432 - val_loss: 0.8824 - val_accuracy: 0.8220\n",
            "Epoch 10/20\n",
            "7982/7982 [==============================] - 1s 132us/step - loss: 0.2085 - accuracy: 0.9500 - val_loss: 0.9336 - val_accuracy: 0.8170\n",
            "Epoch 11/20\n",
            "7982/7982 [==============================] - 1s 134us/step - loss: 0.1940 - accuracy: 0.9496 - val_loss: 0.9178 - val_accuracy: 0.8190\n",
            "Epoch 12/20\n",
            "7982/7982 [==============================] - 1s 133us/step - loss: 0.1681 - accuracy: 0.9516 - val_loss: 0.9619 - val_accuracy: 0.8110\n",
            "Epoch 13/20\n",
            "7982/7982 [==============================] - 1s 130us/step - loss: 0.1575 - accuracy: 0.9538 - val_loss: 0.9529 - val_accuracy: 0.8150\n",
            "Epoch 14/20\n",
            "7982/7982 [==============================] - 1s 132us/step - loss: 0.1380 - accuracy: 0.9548 - val_loss: 1.0211 - val_accuracy: 0.8070\n",
            "Epoch 15/20\n",
            "7982/7982 [==============================] - 1s 131us/step - loss: 0.1348 - accuracy: 0.9569 - val_loss: 0.9830 - val_accuracy: 0.8080\n",
            "Epoch 16/20\n",
            "7982/7982 [==============================] - 1s 131us/step - loss: 0.1337 - accuracy: 0.9544 - val_loss: 0.9654 - val_accuracy: 0.8150\n",
            "Epoch 17/20\n",
            "7982/7982 [==============================] - 1s 133us/step - loss: 0.1175 - accuracy: 0.9577 - val_loss: 1.0509 - val_accuracy: 0.8060\n",
            "Epoch 18/20\n",
            "7982/7982 [==============================] - 1s 131us/step - loss: 0.1248 - accuracy: 0.9580 - val_loss: 1.1339 - val_accuracy: 0.7920\n",
            "Epoch 19/20\n",
            "7982/7982 [==============================] - 1s 131us/step - loss: 0.1139 - accuracy: 0.9578 - val_loss: 1.1980 - val_accuracy: 0.7780\n",
            "Epoch 20/20\n",
            "7982/7982 [==============================] - 1s 131us/step - loss: 0.1120 - accuracy: 0.9583 - val_loss: 1.0700 - val_accuracy: 0.7940\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f04f1d840f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkCLX9PiD9V8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a31161ed-ad5b-4993-9fa7-b3e9f776af37"
      },
      "source": [
        "result = model_4.evaluate(x_test,one_hot_test_labels)\n",
        "print(result)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2246/2246 [==============================] - 0s 99us/step\n",
            "[1.2411063613365085, 0.7889581322669983]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJ1JPiTErD7X",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## We were using two hidden layers. Now try to use a single hidden layer, or three hidden layers, and see if you will be able to improve the model performance on the test data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zukIClyrh3k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 751
        },
        "outputId": "cfc21e3c-e43b-424c-b2a5-291568d734ba"
      },
      "source": [
        "# TODO: Try to use a single hidden layer, or three hidden layers\n",
        "model_5 = models.Sequential()\n",
        "model_5.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
        "model_5.add(layers.Dense(46, activation='softmax')) \n",
        "\n",
        "model_5.compile(optimizer = 'rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model_5.fit(partial_x_train, partial_y_train, epochs=20, batch_size=512, validation_data=(x_val, y_val))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 7982 samples, validate on 1000 samples\n",
            "Epoch 1/20\n",
            "7982/7982 [==============================] - 1s 156us/step - loss: 2.5444 - accuracy: 0.5486 - val_loss: 1.7957 - val_accuracy: 0.6560\n",
            "Epoch 2/20\n",
            "7982/7982 [==============================] - 1s 132us/step - loss: 1.4488 - accuracy: 0.7181 - val_loss: 1.3476 - val_accuracy: 0.7210\n",
            "Epoch 3/20\n",
            "7982/7982 [==============================] - 1s 130us/step - loss: 1.0579 - accuracy: 0.7910 - val_loss: 1.1433 - val_accuracy: 0.7680\n",
            "Epoch 4/20\n",
            "7982/7982 [==============================] - 1s 133us/step - loss: 0.8268 - accuracy: 0.8364 - val_loss: 1.0285 - val_accuracy: 0.7830\n",
            "Epoch 5/20\n",
            "7982/7982 [==============================] - 1s 130us/step - loss: 0.6634 - accuracy: 0.8741 - val_loss: 0.9505 - val_accuracy: 0.8020\n",
            "Epoch 6/20\n",
            "7982/7982 [==============================] - 1s 131us/step - loss: 0.5406 - accuracy: 0.8956 - val_loss: 0.9093 - val_accuracy: 0.8110\n",
            "Epoch 7/20\n",
            "7982/7982 [==============================] - 1s 127us/step - loss: 0.4471 - accuracy: 0.9117 - val_loss: 0.8640 - val_accuracy: 0.8230\n",
            "Epoch 8/20\n",
            "7982/7982 [==============================] - 1s 131us/step - loss: 0.3719 - accuracy: 0.9245 - val_loss: 0.8388 - val_accuracy: 0.8260\n",
            "Epoch 9/20\n",
            "7982/7982 [==============================] - 1s 127us/step - loss: 0.3150 - accuracy: 0.9340 - val_loss: 0.8322 - val_accuracy: 0.8330\n",
            "Epoch 10/20\n",
            "7982/7982 [==============================] - 1s 129us/step - loss: 0.2676 - accuracy: 0.9395 - val_loss: 0.8285 - val_accuracy: 0.8300\n",
            "Epoch 11/20\n",
            "7982/7982 [==============================] - 1s 127us/step - loss: 0.2320 - accuracy: 0.9466 - val_loss: 0.8495 - val_accuracy: 0.8180\n",
            "Epoch 12/20\n",
            "7982/7982 [==============================] - 1s 128us/step - loss: 0.2028 - accuracy: 0.9499 - val_loss: 0.8395 - val_accuracy: 0.8270\n",
            "Epoch 13/20\n",
            "7982/7982 [==============================] - 1s 127us/step - loss: 0.1815 - accuracy: 0.9520 - val_loss: 0.8554 - val_accuracy: 0.8220\n",
            "Epoch 14/20\n",
            "7982/7982 [==============================] - 1s 130us/step - loss: 0.1626 - accuracy: 0.9546 - val_loss: 0.8846 - val_accuracy: 0.8200\n",
            "Epoch 15/20\n",
            "7982/7982 [==============================] - 1s 128us/step - loss: 0.1498 - accuracy: 0.9545 - val_loss: 0.8883 - val_accuracy: 0.8140\n",
            "Epoch 16/20\n",
            "7982/7982 [==============================] - 1s 128us/step - loss: 0.1376 - accuracy: 0.9551 - val_loss: 0.8768 - val_accuracy: 0.8200\n",
            "Epoch 17/20\n",
            "7982/7982 [==============================] - 1s 129us/step - loss: 0.1298 - accuracy: 0.9573 - val_loss: 0.9074 - val_accuracy: 0.8150\n",
            "Epoch 18/20\n",
            "7982/7982 [==============================] - 1s 132us/step - loss: 0.1226 - accuracy: 0.9573 - val_loss: 0.9069 - val_accuracy: 0.8150\n",
            "Epoch 19/20\n",
            "7982/7982 [==============================] - 1s 128us/step - loss: 0.1162 - accuracy: 0.9579 - val_loss: 0.9169 - val_accuracy: 0.8150\n",
            "Epoch 20/20\n",
            "7982/7982 [==============================] - 1s 127us/step - loss: 0.1083 - accuracy: 0.9574 - val_loss: 0.9502 - val_accuracy: 0.8130\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f04f12e55c0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YDTOZOpFZ7C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e3d6bdb1-f890-459c-e459-8a59135b2b53"
      },
      "source": [
        "result = model_5.evaluate(x_test,one_hot_test_labels)\n",
        "print(result)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2246/2246 [==============================] - 0s 94us/step\n",
            "[1.0768302429603447, 0.7902938723564148]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gldwCoKB3oiI",
        "colab_type": "text"
      },
      "source": [
        "## Wrapping up\n",
        "\n",
        "\n",
        "Here's what you should take away from this example:\n",
        "\n",
        "* If you are trying to classify data points between N classes, your network should end with a `Dense` layer of size N.\n",
        "* In a single-label, multi-class classification problem, your network should end with a `softmax` activation, so that it will output a \n",
        "probability distribution over the N output classes.\n",
        "* _Categorical crossentropy_ is almost always the loss function you should use for such problems. It minimizes the distance between the \n",
        "probability distributions output by the network, and the true distribution of the targets.\n",
        "* There are two ways to handle labels in multi-class classification:\n",
        "    ** Encoding the labels via \"categorical encoding\" (also known as \"one-hot encoding\") and using `categorical_crossentropy` as your loss \n",
        "function.\n",
        "    ** Encoding the labels as integers and using the `sparse_categorical_crossentropy` loss function.\n",
        "* If you need to classify data into a large number of categories, then you should avoid creating information bottlenecks in your network by having \n",
        "intermediate layers that are too small."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKekl-HuGXiU",
        "colab_type": "text"
      },
      "source": [
        "## Bonus Point\n",
        "Can you think of other methods to further improve the model performance? Code it up and evaluate them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38I30otcrO-r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 751
        },
        "outputId": "0a2dfaa9-c0a5-43f6-ca27-798c9891818c"
      },
      "source": [
        "# TODO\n",
        "#Try reducing the batch size\n",
        "model_b = models.Sequential()\n",
        "model_b.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
        "model_b.add(layers.Dense(46, activation='relu'))\n",
        "model_b.add(layers.Dense(46, activation='softmax')) \n",
        "\n",
        "model_b.compile( optimizer = 'rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model_b.fit(partial_x_train, partial_y_train, epochs=20, batch_size=64, validation_data=(x_val, y_val))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 7982 samples, validate on 1000 samples\n",
            "Epoch 1/20\n",
            "7982/7982 [==============================] - 2s 304us/step - loss: 1.6754 - accuracy: 0.6493 - val_loss: 1.1200 - val_accuracy: 0.7470\n",
            "Epoch 2/20\n",
            "7982/7982 [==============================] - 2s 260us/step - loss: 0.8577 - accuracy: 0.8127 - val_loss: 0.9225 - val_accuracy: 0.7980\n",
            "Epoch 3/20\n",
            "7982/7982 [==============================] - 2s 275us/step - loss: 0.5514 - accuracy: 0.8822 - val_loss: 0.8345 - val_accuracy: 0.8190\n",
            "Epoch 4/20\n",
            "7982/7982 [==============================] - 2s 270us/step - loss: 0.3698 - accuracy: 0.9206 - val_loss: 0.8597 - val_accuracy: 0.8210\n",
            "Epoch 5/20\n",
            "7982/7982 [==============================] - 2s 265us/step - loss: 0.2745 - accuracy: 0.9390 - val_loss: 0.8654 - val_accuracy: 0.8280\n",
            "Epoch 6/20\n",
            "7982/7982 [==============================] - 2s 261us/step - loss: 0.2248 - accuracy: 0.9474 - val_loss: 0.9357 - val_accuracy: 0.8180\n",
            "Epoch 7/20\n",
            "7982/7982 [==============================] - 2s 271us/step - loss: 0.1899 - accuracy: 0.9516 - val_loss: 0.9621 - val_accuracy: 0.8160\n",
            "Epoch 8/20\n",
            "7982/7982 [==============================] - 2s 267us/step - loss: 0.1728 - accuracy: 0.9521 - val_loss: 1.0270 - val_accuracy: 0.8100\n",
            "Epoch 9/20\n",
            "7982/7982 [==============================] - 2s 261us/step - loss: 0.1579 - accuracy: 0.9539 - val_loss: 1.1381 - val_accuracy: 0.7900\n",
            "Epoch 10/20\n",
            "7982/7982 [==============================] - 2s 263us/step - loss: 0.1499 - accuracy: 0.9553 - val_loss: 1.1731 - val_accuracy: 0.7940\n",
            "Epoch 11/20\n",
            "7982/7982 [==============================] - 2s 265us/step - loss: 0.1371 - accuracy: 0.9551 - val_loss: 1.1155 - val_accuracy: 0.8040\n",
            "Epoch 12/20\n",
            "7982/7982 [==============================] - 2s 260us/step - loss: 0.1363 - accuracy: 0.9559 - val_loss: 1.1294 - val_accuracy: 0.8020\n",
            "Epoch 13/20\n",
            "7982/7982 [==============================] - 2s 256us/step - loss: 0.1288 - accuracy: 0.9572 - val_loss: 1.1909 - val_accuracy: 0.8020\n",
            "Epoch 14/20\n",
            "7982/7982 [==============================] - 2s 271us/step - loss: 0.1210 - accuracy: 0.9565 - val_loss: 1.3041 - val_accuracy: 0.7860\n",
            "Epoch 15/20\n",
            "7982/7982 [==============================] - 2s 263us/step - loss: 0.1183 - accuracy: 0.9573 - val_loss: 1.2472 - val_accuracy: 0.7920\n",
            "Epoch 16/20\n",
            "7982/7982 [==============================] - 2s 262us/step - loss: 0.1113 - accuracy: 0.9594 - val_loss: 1.3903 - val_accuracy: 0.8020\n",
            "Epoch 17/20\n",
            "7982/7982 [==============================] - 2s 261us/step - loss: 0.1106 - accuracy: 0.9569 - val_loss: 1.3949 - val_accuracy: 0.7950\n",
            "Epoch 18/20\n",
            "7982/7982 [==============================] - 2s 256us/step - loss: 0.1055 - accuracy: 0.9580 - val_loss: 1.4574 - val_accuracy: 0.7880\n",
            "Epoch 19/20\n",
            "7982/7982 [==============================] - 2s 258us/step - loss: 0.1025 - accuracy: 0.9594 - val_loss: 1.4731 - val_accuracy: 0.7920\n",
            "Epoch 20/20\n",
            "7982/7982 [==============================] - 2s 266us/step - loss: 0.1019 - accuracy: 0.9573 - val_loss: 1.4957 - val_accuracy: 0.7900\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f04f0d4aa90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35Eb8Gk8GxEN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "6f690aa6-1c3a-4759-a63b-de6953cc37fc"
      },
      "source": [
        "result = model_b.evaluate(x_test,one_hot_test_labels)\n",
        "print(result)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2246/2246 [==============================] - 0s 93us/step\n",
            "[1.831015934608711, 0.7742653489112854]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHZpgHjiG8B3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "c6b59d74-85d3-433b-c6ca-670865701dfe"
      },
      "source": [
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "acc = history.history['categorical_accuracy']\n",
        "val_acc = history.history['val_categorical_accuracy']\n",
        "\n",
        "batch = range(64, len(acc) +64)\n",
        "\n",
        "\n",
        "plt.plot(batch, loss, 'bo', label='Training loss')\n",
        "plt.plot(batch, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Batch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f04f0844400>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zw_odtnxHAZH",
        "colab_type": "text"
      },
      "source": [
        "Loss gets smaller when batch size is reduced, moving close to 0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7CGM-0QrOrp",
        "colab_type": "text"
      },
      "source": [
        "# End of HW1"
      ]
    }
  ]
}